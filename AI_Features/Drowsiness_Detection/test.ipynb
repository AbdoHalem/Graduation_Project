{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import playsound\n",
    "from scipy.spatial import distance\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open shape_predictor_68_face_landmarks.dat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7de850b8df1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7de850b8df1c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Initialize the face detector from dlib to detect frontal faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSHAPE_PREDICTOR_PATH\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Load the facial landmark predictor to estimate key facial landmarks on detected faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mleft_eye_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_eye_end\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFACIAL_LANDMARKS_68_IDXS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"left_eye\"\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Get indices for the left eye landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mright_eye_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_eye_end\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFACIAL_LANDMARKS_68_IDXS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"right_eye\"\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Get indices for the right eye landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
     ]
    }
   ],
   "source": [
    "# Constants for thresholds and frame limits\n",
    "EYE_ASPECT_RATIO_THRESHOLD = 0.2  # The EAR value below which the driver is considered drowsy\n",
    "YAWN_RATIO_THRESHOLD = 0.5  # The Yawn ratio above which the driver is considered yawning\n",
    "DROWSINESS_FRAME_THRESHOLD = 15  # Number of frames to trigger drowsiness warning\n",
    "NO_FACE_FRAME_THRESHOLD = 25  # Number of frames to trigger no-face warning\n",
    "\n",
    "# Paths to resources\n",
    "SHAPE_PREDICTOR_PATH = \"shape_predictor_68_face_landmarks.dat\"\n",
    "LOOK_AHEAD_SOUND = \"look_in_front_of_you.mp3\"\n",
    "OPEN_EYES_SOUND = \"open_your_eyes.mp3\"\n",
    "YAWN_WARNING_SOUND = \"you_are_yawning.mp3\"\n",
    "\n",
    "# counters\n",
    "drowsiness_frame_counter = 0 # counter for drowsy frames\n",
    "no_face_detected_counter = 0  # counter for frames with no face detected\n",
    "\n",
    "# Calculate the Eye Aspect Ratio (EAR) for a given eye for param eye which is an Array of eye landmarks\n",
    "def calculate_ear(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2 * C)\n",
    "\n",
    "# Calculate the yawn ratio for a given mouth with param mouth which is an Array of mouth landmarks\n",
    "def calculate_yawn_ratio(mouth):\n",
    "    D = distance.euclidean(mouth[1], mouth[7])\n",
    "    E = distance.euclidean(mouth[3], mouth[5])\n",
    "    F = distance.euclidean(mouth[0], mouth[4])\n",
    "    return (D + E) / (2 * F)\n",
    "\n",
    "def play_alarm(sound_file):\n",
    "    playsound.playsound(sound_file, True)\n",
    "\n",
    "# Check if the driver is drowsy or yawning and trigger warnings if necessary.\n",
    "def detect_drowsiness(frame, average_ear, yawn_ratio):\n",
    "\n",
    "    global drowsiness_frame_counter\n",
    "    \n",
    "    if average_ear < EYE_ASPECT_RATIO_THRESHOLD or yawn_ratio > YAWN_RATIO_THRESHOLD: # Check if the driver is drowsy based on EAR or yawn ratio\n",
    "        drowsiness_frame_counter += 1 # Increment the counter for consecutive drowsy frames\n",
    "        if drowsiness_frame_counter > DROWSINESS_FRAME_THRESHOLD: \n",
    "            cv2.putText(frame, \"You are drowsy!!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            if average_ear < EYE_ASPECT_RATIO_THRESHOLD:\n",
    "                play_alarm(OPEN_EYES_SOUND)\n",
    "            if yawn_ratio > YAWN_RATIO_THRESHOLD:\n",
    "                play_alarm(YAWN_WARNING_SOUND)\n",
    "    else:\n",
    "        drowsiness_frame_counter = 0  # Reset counter if no drowsiness detected\n",
    "\n",
    "# Handle the case where no face is detected\n",
    "def no_face_detection(frame, face_detected):\n",
    "   \n",
    "    global no_face_detected_counter\n",
    "\n",
    "    if not face_detected: # Check if no face is detected\n",
    "        no_face_detected_counter += 1\n",
    "        if no_face_detected_counter > NO_FACE_FRAME_THRESHOLD:\n",
    "            cv2.putText(frame, \"Look in front of you!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "            play_alarm(LOOK_AHEAD_SOUND)\n",
    "    else:\n",
    "        no_face_detected_counter = 0  # Reset counter if a face is detected\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    detector = dlib.get_frontal_face_detector()  # Initialize the face detector from dlib to detect frontal faces\n",
    "    predictor = dlib.shape_predictor(SHAPE_PREDICTOR_PATH) # Load the facial landmark predictor to estimate key facial landmarks on detected faces\n",
    "    (left_eye_start, left_eye_end) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]  # Get indices for the left eye landmarks\n",
    "    (right_eye_start, right_eye_end) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]  # Get indices for the right eye landmarks\n",
    "    (inner_mouth_start, inner_mouth_end) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"inner_mouth\"]  # Get indices for the inner mouth landmarks\n",
    "\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale for improved processing efficiency in face detection\n",
    "            faces_detected = detector(gray) # Detect faces in the grayscale image\n",
    "\n",
    "            # Handle no-face detection\n",
    "            no_face_detection(frame, face_detected=(len(faces_detected) > 0))\n",
    "\n",
    "            for face in faces_detected:\n",
    "                # Draw a rectangle around the detected face on the frame\n",
    "                x1 = face.left()\n",
    "                y1 = face.top()\n",
    "                x2 = face.right()\n",
    "                y2 = face.bottom()\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "\n",
    "                landmarks = predictor(gray, face)  # Predict facial landmarks for the detected face in the grayscale image\n",
    "                landmarks = face_utils.shape_to_np(landmarks)  # Convert landmarks to a NumPy array for easier manipulation\n",
    "\n",
    "                # Extract eyes and mouth landmarks\n",
    "                left_eye = landmarks[left_eye_start:left_eye_end]\n",
    "                right_eye = landmarks[right_eye_start:right_eye_end]\n",
    "                inner_mouth = landmarks[inner_mouth_start:inner_mouth_end]\n",
    "\n",
    "                # Calculate EAR and Yawn ratios\n",
    "                left_ear = calculate_ear(left_eye)\n",
    "                right_ear = calculate_ear(right_eye)\n",
    "                average_ear = (left_ear + right_ear) / 2\n",
    "                yawn_ratio = calculate_yawn_ratio(inner_mouth)\n",
    "\n",
    "                # Draw contours for eyes and mouth\n",
    "                cv2.drawContours(frame, [cv2.convexHull(left_eye)], -1, (0, 255, 0), 1)\n",
    "                cv2.drawContours(frame, [cv2.convexHull(right_eye)], -1, (0, 255, 0), 1)\n",
    "                cv2.drawContours(frame, [cv2.convexHull(inner_mouth)], -1, (0, 255, 0), 1)\n",
    "\n",
    "                # Detect drowsiness\n",
    "                detect_drowsiness(frame, average_ear, yawn_ratio)\n",
    "\n",
    "            cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Abdo_Halem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
